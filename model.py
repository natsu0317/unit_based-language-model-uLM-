from transformers import GPT2Config, GPT2LMHeadModel
import torch

def create_unit_language_model():
    config = GPT2Config(
        vocab_size=128,
        n_positions=512,
        n_embd=512, # åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ
        n_layer=6, # Transformerå±¤æ•°
        n_head=8, # attentionheadæ•°
        n_inner=2048,
        activation_function="gelu",
    )

    model = GPT2LMHeadModel(config)
    return model, config

def test_model():
    model,config = create_unit_language_model()
    
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    model = model.to(device)
    model.eval()
    test_input = torch.tensor([[15, 42, 67, 23, 89]], device=device) 
    with torch.no_grad():
        outputs = model(test_input)
        logits = outputs.logits
    last_logits = logits[0, -1, :]  # æœ€å¾Œã®ä½ç½®ã®äºˆæ¸¬
    probabilities = torch.softmax(last_logits, dim=-1)
    top_probs, top_indices = torch.topk(probabilities, 5)
    
    print(f"\\nğŸ¯ æ¬¡ã®éŸ³éŸ¿å˜ä½ã®äºˆæ¸¬ï¼ˆä¸Šä½5å€‹ï¼‰:")
    for i, (prob, idx) in enumerate(zip(top_probs, top_indices)):
        print(f"  {i+1}ä½: Unit {idx.item()} (ç¢ºç‡: {prob.item():.4f})")
    
    print("\\nâœ… ãƒ¢ãƒ‡ãƒ«ãƒ†ã‚¹ãƒˆå®Œäº†!")

if __name__ == "__main__":
    test_model()