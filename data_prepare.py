import pandas as pd
import numpy as np
import torch
from torch.utils.data import Dataset, DataLoader
from transformers import (
    AutoConfig,
    AutoModelForCausalLM,
    AutoTokenizer,
    TrainingArguments,
    Trainer,
    GPT2Config,
    GPT2LMHeadModel,
)
import time

class UnitDataset(Dataset):
    def __init__(self, csv_file, max_length=1024):
        self.df = pd.read_csv(csv_file)
        self.max_length = max_length

    def __len__(self):
        return len(self.df)

    def __getitem__(self,idx):
        units_str = self.df.iloc[idx]['text']
        sequence = [int(x) for x in units_str.split()]

        sequence = sequence[:self.max_length+1]

        input_ids = sequence[:-1]  # æœ€å¾Œã‚’é™¤ã
        labels = sequence[1:]      # æœ€åˆã‚’é™¤ã
        
        return {
            'input_ids': torch.tensor(input_ids, dtype=torch.long),
            'labels': torch.tensor(labels, dtype=torch.long)
        }
    
def create_collate_fn(pad_token_id=0):
    
    def collate_fn(batch):
        # ãƒãƒƒãƒå†…ã®æœ€å¤§é•·ã‚’å–å¾—
        max_input_len = max(len(item['input_ids']) for item in batch)
        max_label_len = max(len(item['labels']) for item in batch)
        max_len = max(max_input_len, max_label_len)
        
        # ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°å‡¦ç†
        padded_inputs = []
        padded_labels = []
        attention_masks = []
        
        for item in batch:
            input_ids = item['input_ids']
            labels = item['labels']
            
            # å…¥åŠ›ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
            input_padded = torch.full((max_len,), pad_token_id, dtype=torch.long)
            input_padded[:len(input_ids)] = input_ids
            padded_inputs.append(input_padded)
            
            # ãƒ©ãƒ™ãƒ«ãƒ‘ãƒ‡ã‚£ãƒ³ã‚°
            label_padded = torch.full((max_len,), -100, dtype=torch.long)
            label_padded[:len(labels)] = labels
            padded_labels.append(label_padded)
            
            # ã‚¢ãƒ†ãƒ³ã‚·ãƒ§ãƒ³ãƒã‚¹ã‚¯
            mask = torch.zeros(max_len, dtype=torch.long)
            mask[:len(input_ids)] = 1
            attention_masks.append(mask)
        
        return {
            'input_ids': torch.stack(padded_inputs),
            'labels': torch.stack(padded_labels),
            'attention_mask': torch.stack(attention_masks)
        }
    
    return collate_fn


def create_dataloaders(train_csv, dev_csv, batch_size=8, max_length=1024):
    
    train_dataset = UnitDataset(train_csv, max_length=max_length)
    dev_dataset = UnitDataset(dev_csv, max_length=max_length)

    collate_fn = create_collate_fn(pad_token_id=0)

    train_loader = DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        collate_fn=collate_fn,
        num_workers=2, #ä¸¦åˆ—å‡¦ç†
        pin_memory=False,
    )

    dev_loader = DataLoader(
        dev_dataset,
        batch_size=batch_size,
        shuffle=False,
        collate_fn=collate_fn,
        num_workers=2, #ä¸¦åˆ—å‡¦ç†
        pin_memory=False,
    )

    return train_loader, dev_loader


def create_unit_language_model():
    config = GPT2Config(
        vocab_size=128,
        n_positions=1024, # æœ€å¤§ç³»åˆ—é•·ï¼ˆè«–æ–‡ã¯3072)
        n_embd=1024, # åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒ
        n_layer=12, # Transformerå±¤æ•°
        n_head=16, # attentionheadæ•°
        n_inner=4096,

        # éå­¦ç¿’é˜²æ­¢
        resid_pdrop=0.1,
        embd_pdrop=0.1,
        attn_pdrop=0.1,
    )

    model = GPT2LMHeadModel(config)
    return model, config


def create_qwen_unit_lm(vocab_size=128, model_name="Qwen/Qwen2.5-3B"):
    
    original_model = AutoModelForCausalLM.from_pretrained(model_name)
    original_config = original_model.config
    
    
    new_config = AutoConfig.from_pretrained(model_name)
    new_config.vocab_size = vocab_size  # 151,936->128ã«å¤‰æ›´
    # new_config.max_position_embeddings = 1024

    new_model = AutoModelForCausalLM.from_config(new_config)

    
    original_state_dict = original_model.state_dict()
    new_state_dict = new_model.state_dict()



    return model, 
    copied_layers = 0
    skipped_layers = 0
    
    for name, param in original_state_dict.items():
        if name in new_state_dict:
            if param.shape == new_state_dict[name].shape:
                # å½¢çŠ¶ãŒåŒã˜å ´åˆã¯ã‚³ãƒ”ãƒ¼
                new_state_dict[name].copy_(param)
                copied_layers += 1
            else:
                # å½¢çŠ¶ãŒé•ã†å ´åˆï¼ˆä¸»ã«åŸ‹ã‚è¾¼ã¿å±¤ï¼‰ã¯ã‚¹ã‚­ãƒƒãƒ—
                print(f"  âš ï¸ ã‚¹ã‚­ãƒƒãƒ—: {name} (å½¢çŠ¶ä¸ä¸€è‡´)")
                skipped_layers += 1
        else:
            print(f"  âš ï¸ è¦‹ã¤ã‹ã‚‰ãªã„: {name}")
    
    print(f"  âœ… ã‚³ãƒ”ãƒ¼å®Œäº†: {copied_layers} å±¤")
    print(f"  âš ï¸ ã‚¹ã‚­ãƒƒãƒ—: {skipped_layers} å±¤")
    
    # === Step 5: åŸ‹ã‚è¾¼ã¿å±¤ã®åˆæœŸåŒ– ===
    print("ğŸ² åŸ‹ã‚è¾¼ã¿å±¤ã‚’åˆæœŸåŒ–...")
    
    # æ–°ã—ã„åŸ‹ã‚è¾¼ã¿å±¤ã‚’é©åˆ‡ã«åˆæœŸåŒ–
    if hasattr(new_model, 'transformer') and hasattr(new_model.transformer, 'wte'):
        # GPTç³»ã®å ´åˆ
        embedding_layer = new_model.transformer.wte
    elif hasattr(new_model, 'model') and hasattr(new_model.model, 'embed_tokens'):
        # Qwenç³»ã®å ´åˆ
        embedding_layer = new_model.model.embed_tokens
    else:
        print("  âš ï¸ åŸ‹ã‚è¾¼ã¿å±¤ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“")
        embedding_layer = None
    
    if embedding_layer is not None:
        # XavieråˆæœŸåŒ–
        torch.nn.init.xavier_uniform_(embedding_layer.weight)
        print(f"  âœ… åŸ‹ã‚è¾¼ã¿å±¤åˆæœŸåŒ–å®Œäº†: {embedding_layer.weight.shape}")
    
    print("âœ… ãƒ¢ãƒ‡ãƒ«ä½œæˆå®Œäº†!")

    return new_model, new_config

def setup_training(model, train_dataset, dev_dataset):
    training_args = TrainingArguments(
        output_dir="./qwen-unit-lm",
        overwrite_output_dir=True,

        num_train_epochs=10,
        per_device_train_batch_size=32,  # Qwenã¯å¤§ãã„ã®ã§ãƒãƒƒãƒã‚µã‚¤ã‚ºå°ã•ã‚
        per_device_eval_batch_size=32,
        gradient_accumulation_steps=8,   # å®ŸåŠ¹ãƒãƒƒãƒã‚µã‚¤ã‚º = 4 * 8 = 32
        
        # æœ€é©åŒ–
        learning_rate=5e-5,  # Qwenã¯å°ã•ã‚ã®å­¦ç¿’ç‡
        weight_decay=0.01,
        warmup_steps=1000,
        
        # è©•ä¾¡ãƒ»ä¿å­˜
        evaluation_strategy="steps",
        eval_steps=500,
        save_strategy="steps", 
        save_steps=1000,
        save_total_limit=3,
        
        # ãƒ­ã‚°
        logging_steps=100,
        report_to=None,  # wandbãªã©ã‚’ä½¿ã‚ãªã„å ´åˆ
        
        # ãƒ¡ãƒ¢ãƒªåŠ¹ç‡åŒ–
        dataloader_pin_memory=False,
        dataloader_num_workers=0,
    )

    trainer = Trainer(
        model=model,
        args=training_args,
        train_dataset=train_dataset,
        eval_dataset=dev_dataset,
    )

    return trainer


def train_unit_language_model():
    """Unit Language Modelã®å­¦ç¿’"""
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"ğŸš€ Using device: {device}")
    
    if torch.cuda.is_available():
        print(f"GPU: {torch.cuda.get_device_name(0)}")
        print(f"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1024**3:.1f} GB")
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆ
    print("ğŸ“š ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆä½œæˆä¸­...")
    train_dataset = UnitDataset('units_train.csv', max_length=1024)
    dev_dataset = UnitDataset('units_dev.csv', max_length=1024)
    print(f"Train samples: {len(train_dataset)}")
    print(f"Dev samples: {len(dev_dataset)}")
    
    # ãƒ¢ãƒ‡ãƒ«ä½œæˆ
    print("ğŸ¤– ãƒ¢ãƒ‡ãƒ«ä½œæˆä¸­...")
    model, config = create_qwen_unit_lm(
        vocab_size=128,
        model_name="Qwen/Qwen2.5-3B"
    )
    model.to(device)
    
    # å­¦ç¿’è¨­å®š
    print("âš™ï¸ å­¦ç¿’è¨­å®šä¸­...")
    trainer = setup_training(model, train_dataset, dev_dataset)
    
    # å­¦ç¿’å‰è©•ä¾¡
    print("ğŸ“Š å­¦ç¿’å‰è©•ä¾¡...")
    eval_results = trainer.evaluate()
    print(f"å­¦ç¿’å‰ã®æå¤±: {eval_results['eval_loss']:.4f}")
    print(f"å­¦ç¿’å‰ã®Perplexity: {np.exp(eval_results['eval_loss']):.2f}")
    
    # å­¦ç¿’é–‹å§‹
    print("ğŸš€ å­¦ç¿’é–‹å§‹...")
    start_time = time.time()
    
    trainer.train()
    
    end_time = time.time()
    training_time = end_time - start_time
    print(f"\\nå­¦ç¿’å®Œäº† (æ‰€è¦æ™‚é–“: {training_time/60:.1f}åˆ†)")  # ä¿®æ­£: \\n â†’ \n
    
    # æœ€çµ‚è©•ä¾¡
    print("\\næœ€çµ‚è©•ä¾¡...")
    final_eval = trainer.evaluate()
    print(f"æœ€çµ‚æå¤±: {final_eval['eval_loss']:.4f}")
    print(f"æœ€çµ‚Perplexity: {np.exp(final_eval['eval_loss']):.2f}")
    
    # ãƒ¢ãƒ‡ãƒ«ä¿å­˜
    print("ğŸ’¾ ãƒ¢ãƒ‡ãƒ«ä¿å­˜ä¸­...")
    trainer.save_model("./qwen-unit-lm-final")
    print("âœ… å­¦ç¿’å®Œäº†!")
    
    return trainer, model


def quick_test():
    max_length = 1024
    
    # ãƒ‡ãƒ¼ã‚¿ã‚»ãƒƒãƒˆãƒ†ã‚¹ãƒˆ
    dataset = UnitDataset('units_train.csv', max_length=max_length)
    
    # ã‚µãƒ³ãƒ—ãƒ«ãƒ‡ãƒ¼ã‚¿ç¢ºèª
    sample = dataset[0]
    
    # ãƒ¢ãƒ‡ãƒ«ä½œæˆãƒ†ã‚¹ãƒˆ
    model, config = create_unit_language_model()
    print(f"  vocab_size: {config.vocab_size}")
    print(f"  n_positions: {config.n_positions}")
    
    # collate_fn ãƒ†ã‚¹ãƒˆ
    collate_fn = create_collate_fn(pad_token_id=0)
    batch = [dataset[i] for i in range(3)]
    collated = collate_fn(batch)
    print(f"  ãƒãƒƒãƒã‚µã‚¤ã‚º: {collated['input_ids'].shape}")
    

if __name__ == "__main__":
    print("ğŸš€ Unit-based Language Model")


    trainer, model = train_unit_language_model()

    
