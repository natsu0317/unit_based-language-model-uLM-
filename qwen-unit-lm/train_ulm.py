# train_ulm.pyprint("=== THIS IS THE NEW VERSION ===")   # â†1è¡Œç›®ã«è¿½åŠ 
import os
import time, torch, numpy as np
from transformers import TrainingArguments, Trainer
from dataset import UnitDataset, collate_fn
from model   import build_model

DEVICE = "cuda" if torch.cuda.is_available() else "cpu"

print("ğŸ“š  Loading datasets â€¦")
train_ds = UnitDataset("units_train.csv")
dev_ds   = UnitDataset("units_dev.csv")

print("ğŸ¤–  Building model â€¦")
model = build_model().to(DEVICE)

args = TrainingArguments(
    output_dir="ulm-qwen3b",
    overwrite_output_dir=True,
    bf16=True,
    gradient_checkpointing=True,
    gradient_checkpointing_kwargs={"use_reentrant": False},
    num_train_epochs=3,
    per_device_train_batch_size=16,
    per_device_eval_batch_size=16,
    gradient_accumulation_steps=4,     # effective 64
    learning_rate=2e-4,
    warmup_steps=2000,
    weight_decay=0.01,
    evaluation_strategy="steps",
    eval_steps=1000,
    save_strategy="steps",
    save_steps=1000,
    logging_steps=100,
    max_grad_norm=1.0,
    report_to="none",
    dataloader_num_workers=4,
    dataloader_pin_memory=True,
)

trainer = Trainer(
    model=model,
    args=args,
    train_dataset=train_ds,
    eval_dataset=dev_ds,
    data_collator=lambda x: collate_fn(x, pad_id=0),
)

print("ğŸ”  Evaluating before training â€¦")
# m0 = trainer.evaluate()
# print(f"   PPL(before) : {np.exp(m0['eval_loss']):.2f}")

t0 = time.time()
checkpoint = "ulm-qwen3b/checkpoint-9000"

if checkpoint and os.path.isdir(checkpoint):
    trainer.train(resume_from_checkpoint=checkpoint)
else:
    trainer.train()
print(f"â±  Training time : {(time.time()-t0)/60:.1f} min")

m1 = trainer.evaluate()
print(f"   PPL(after)  : {np.exp(m1['eval_loss']):.2f}")

# LoRA ã‚¢ãƒ€ãƒ—ã‚¿ã®ã¿ (å†å­¦ç¿’ç”¨)
trainer.save_model("ulm-qwen3b-lora")

# LoRA ã‚’ãƒ™ãƒ¼ã‚¹ã«çµ±åˆã—ã¦ä¿å­˜ (æ¨è«–ç”¨)
print("ğŸ’¾  Merging LoRA and saving â€¦")
merged = trainer.model.merge_and_unload()
merged.save_pretrained("ulm-qwen3b-merged")